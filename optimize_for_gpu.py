#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Mangio RVC –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é GPU
"""

import torch
import os
import json
from pathlib import Path

class GPUOptimizer:
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.gpu_name = None
        self.gpu_memory = 0
        self.optimizations = {}
        
        if self.gpu_available:
            self.gpu_name = torch.cuda.get_device_name(0)
            self.gpu_memory = torch.cuda.get_device_properties(0).total_memory // (1024**3)
            print(f"üéÆ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ GPU: {self.gpu_name} ({self.gpu_memory}GB)")
        else:
            print("‚ö†Ô∏è  GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU —Ä–µ–∂–∏–º")
    
    def detect_gpu_capabilities(self):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π GPU"""
        if not self.gpu_available:
            return
        
        compute_capability = torch.cuda.get_device_capability(0)
        self.optimizations['compute_capability'] = f"{compute_capability[0]}.{compute_capability[1]}"
        
        try:
            import tensorrt
            self.optimizations['tensorrt'] = True
            print("‚úÖ TensorRT –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
        except:
            self.optimizations['tensorrt'] = False
            
        if compute_capability[0] >= 8:
            self.optimizations['flash_attention'] = True
            print("‚úÖ Flash Attention –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
        else:
            self.optimizations['flash_attention'] = False
    
    def optimize_for_gpu(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π GPU"""
        
        os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
        os.environ["CUDNN_BENCHMARK"] = "1"
        
        if self.gpu_name and ("H100" in self.gpu_name or "H800" in self.gpu_name):
            print("üöÄ –ü—Ä–∏–º–µ–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è H100/H800")
            os.environ["TORCH_CUDA_ARCH_LIST"] = "9.0"
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
            os.environ["CUDA_DEVICE_MAX_CONNECTIONS"] = "1"
            batch_size = 128
            num_workers = 16
            
        elif self.gpu_name and ("A100" in self.gpu_name or "A6000" in self.gpu_name):
            print("üöÄ –ü—Ä–∏–º–µ–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è A100/A6000")
            os.environ["TORCH_CUDA_ARCH_LIST"] = "8.0;8.6"
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
            batch_size = 64 if self.gpu_memory >= 40 else 32
            num_workers = 12
            
        elif self.gpu_name and ("4090" in self.gpu_name or "4080" in self.gpu_name):
            print("üöÄ –ü—Ä–∏–º–µ–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 40 —Å–µ—Ä–∏–∏")
            os.environ["TORCH_CUDA_ARCH_LIST"] = "8.9"
            os.environ["TORCH_CUDNN_V8_API_ENABLED"] = "1"
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:256"
            batch_size = 32
            num_workers = 8
            
        elif self.gpu_name and ("3090" in self.gpu_name or "3080" in self.gpu_name):
            print("üöÄ –ü—Ä–∏–º–µ–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 30 —Å–µ—Ä–∏–∏")
            os.environ["TORCH_CUDA_ARCH_LIST"] = "8.6"
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
            batch_size = 16
            num_workers = 6
            
        elif self.gpu_name and "V100" in self.gpu_name:
            print("üöÄ –ü—Ä–∏–º–µ–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è V100")
            os.environ["TORCH_CUDA_ARCH_LIST"] = "7.0"
            batch_size = 16
            num_workers = 4
            
        else:
            print("üöÄ –ü—Ä–∏–º–µ–Ω—è—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
            batch_size = 8
            num_workers = 4
        
        config = {
            "gpu_name": self.gpu_name,
            "gpu_memory": self.gpu_memory,
            "batch_size": batch_size,
            "num_workers": num_workers,
            "optimizations": self.optimizations,
            "environment": {k: v for k, v in os.environ.items() if k.startswith(("CUDA", "TORCH", "PYTORCH"))}
        }
        
        with open("gpu_optimization_config.json", "w") as f:
            json.dump(config, f, indent=2)
        
        print(f"\nüìä –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
        print(f"   ‚Ä¢ Batch size: {batch_size}")
        print(f"   ‚Ä¢ Num workers: {num_workers}")
        print(f"   ‚Ä¢ GPU –ø–∞–º—è—Ç—å: {self.gpu_memory}GB")
        
        return batch_size, num_workers
    
    def optimize_model_configs(self, batch_size):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–µ–π"""
        config_files = ["32k.json", "40k.json", "48k.json"]
        configs_dir = Path("configs")
        
        if not configs_dir.exists():
            configs_dir = Path("configs_v2")
        
        for config_file in config_files:
            config_path = configs_dir / config_file
            if config_path.exists():
                with open(config_path, "r") as f:
                    config = json.load(f)
                
                if "train" in config:
                    config["train"]["batch_size"] = batch_size
                    if self.gpu_memory >= 24:
                        config["train"]["fp16_run"] = True
                    
                    if batch_size >= 32:
                        config["train"]["learning_rate"] = 0.0002
                
                with open(config_path, "w") as f:
                    json.dump(config, f, indent=2)
                
                print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω {config_file}")

def main():
    print("üîß –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GPU –¥–ª—è Mangio RVC\n")
    
    optimizer = GPUOptimizer()
    optimizer.detect_gpu_capabilities()
    batch_size, num_workers = optimizer.optimize_for_gpu()
    optimizer.optimize_model_configs(batch_size)
    
    print("\n‚ú® –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("   –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ gpu_optimization_config.json")

if __name__ == "__main__":
    main()
